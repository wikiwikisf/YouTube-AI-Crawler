name: Weekly AI News Crawler

on:
  schedule:
    # Run every Monday at 9:00 AM UTC (adjust timezone as needed)
    - cron: '0 9 * * 1'
  
  # Allow manual triggering
  workflow_dispatch:

jobs:
  crawl-and-publish:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run AI News Crawler
      env:
        YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        EMAIL_USER: ${{ secrets.EMAIL_USER }}
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
        EMAIL_RECIPIENTS: ${{ secrets.EMAIL_RECIPIENTS }}
        SMTP_SERVER: ${{ secrets.SMTP_SERVER }}
        SMTP_PORT: ${{ secrets.SMTP_PORT }}
      run: |
        python src/YouTube_crawler.py
    
    - name: Upload HTML Report as Artifact
      uses: actions/upload-artifact@v4
      with:
        name: weekly-ai-report
        path: ai_news_weekly_*.html
        retention-days: 30
    
    - name: Prepare for GitHub Pages
      run: |
        # Create pages directory
        mkdir -p pages
    
        # Find the latest report file
        LATEST_REPORT=$(ls -t ai_news_weekly_*.html | head -n1)
        echo "ðŸ“„ Latest report: $LATEST_REPORT"
    
        # Copy it as index.html (this becomes your main page)
        cp "$LATEST_REPORT" pages/index.html
    
        echo "âœ… Created index.html from $LATEST_REPORT"

    - name: Upload Pages Artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./pages

    - name: Deploy to GitHub Pages
      uses: actions/deploy-pages@v4
      id: deployment
